include "hosts.conf"

app.path.systems = "/opt/systems"

env {
    cgroups.user = "e2data-partners"
    cgroups.mount-path = "/sys/fs/cgroup"
    cgroups.hierarchy = "yarn"
    accelerator.resource = "yarn.io/gpu-geforcegtx10606gb"
}

system {
  default {
    config {
      master = "cognito"
      masters = [ ${system.default.config.master} ]
      slaves = [ ${system.default.config.master} ]
    }
    path {
      tmp = "/tmp"
      isShared = true
    }
  }
 dstat {
    path {
      home = ${app.path.systems}"/dstat-0.7.3"
      log = ${system.dstat.path.home}"/log"
    }
  }
  flink {
    config {
      yaml {
        jobmanager.rpc.address = cognito
        taskmanager.memory.process.size = 32768m
        jobmanager.memory.process.size = 1024m
        taskmanager.tmp.dirs = ${system.default.path.tmp}"/flink-tmp"
        haier.rest.url = "http://cognito:8080/e2data/flink-schedule"
      }
    }
  }
  hadoop-3 {
    # disable copying of configuration because ICCS nodes have different GPUs
    path.isShared = false
    config {
      core {
        fs.default.name = "hdfs://"${system.default.config.master}":9000"
      }
      yarn {
        yarn.scheduler.maximum-allocation-vcores = 8
        yarn.scheduler.maximum-allocation-mb = 32768
        yarn.scheduler.minimim-allocation-mb = 256
        yarn.resourcemanager.hostname = ${system.default.config.master}
        yarn.nodemanager.resource.memory-mb = 49152
        yarn.nodemanager.resource.cpu-vcores = 16
        yarn.nodemanager.vmem-pmem-ratio = "4"
        yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage = 90
      }
    }
  }
}

experiment.timeout = 1800